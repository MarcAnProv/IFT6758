{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JADti2eq64wD"
   },
   "source": [
    "# Unsupervised Learning\n",
    "# IFT6758 Fall 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzHMPwOw5gT7"
   },
   "source": [
    "## Clustering\n",
    "\n",
    "\n",
    "\n",
    "2. [ISLR 10.7.4] Suppose that for a particular data set, we perform hierarchical clustering using single inkage and using complete linkage. We obtain two dendrograms.\n",
    "\n",
    "  a. At a certain point on the single linkage dendrogram, the clusters $\\{1, 2, 3\\}$ and $\\{4, 5\\}$ fuse. On the complete linkage dendrogram, the clusters $\\{1, 2,3 \\}$ and $\\{4, 5\\}$ also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?\n",
    "  \n",
    "  b. At a certain point on the single linkage dendrogram, the clusters $\\{5\\}$ and $\\{6\\}$ fuse. On the complete linkage dendrogram, the clusters $\\{5\\}$ and $\\{6\\}$ also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "We do not have enough information to tell, because the maximal intercluster dissimilarity (complete linkage) could be equal to the minimal interculster dissimilarity (single linkage). In the case where they would not be equal, the single linkage would occur at a lower point. While if they were equal, they would occur at the same height. \n",
    "### b)\n",
    "They would fuse at the same height, because clusters 5 and 6 are leaves and linkage only affects groups of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knTutpPX60mn"
   },
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fgnYyTP67bi"
   },
   "source": [
    "4. Consider the numbers $\\lambda_1 = \\|z_1\\|_{2},\\dots, \\lambda_{p} = \\|z_{p}\\|_{2}$, giving the proportion of variance explained, as in equation ISLR (10.7). Define the statistics $\\sum_{k = 1}^{p} \\left(\\lambda_k - \\bar{\\lambda}\\right)^2$, where $\\bar{\\lambda}$ is the average of all the $\\lambda_1, \\dots, \\lambda_p$. Discuss the relative usefulness of dimensionality reduction when this statistic is large vs. small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic can be defined as the sum of the squared difference of the amount of variance each principal component is explaining.\n",
    "\n",
    "When the statistic is large, it means that there is a lot of difference between the variance each principal component is explaining. \n",
    "Thus, there exists projections that would reduce the number of dimensions without losing too much information from our initial input space. In this case, PCA would be a good option for dimensionality reduction, because we would be able to find a low-dimensional representation of a data set while retaining a good amount of variation. \n",
    "When the statistic is small PCA wouldn't be a good choice, because we would lose too much information by reducing our initial input space.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "problem_sheet_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
